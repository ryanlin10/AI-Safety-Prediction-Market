[
  {
    "market_title": "By 2025-10-01, will the activation patching of safety neurons in LLMs lead to a measurable improvement in safety performance by at least 90% across three distinct red-teaming benchmarks?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.75,
    "ask_price": 0.82,
    "paper_url": "http://arxiv.org/abs/2406.14144v2",
    "paper_title": "Towards Understanding Safety Alignment: A Mechanistic Perspective from\n  Safety Neurons",
    "pdf_url": "http://arxiv.org/pdf/2406.14144v2",
    "safety_reasoning": "This paper is relevant to AI safety as it identifies specific neurons responsible for safety behaviors in LLMs, which can help mitigate risks associated with harmful content generation. Understanding and manipulating these safety neurons can significantly enhance the alignment of AI systems with human values.",
    "price_reasoning": "The bid price reflects a strong confidence in the findings of the paper, given the experimental results showing a consistent identification of safety neurons and their impact on safety performance. The ask price accounts for some uncertainty regarding the generalizability of these results across different models and benchmarks.",
    "keywords": "alignment, interpretability, mechanistic interpretability, safety"
  },
  {
    "market_title": "By 2025-10-01, will the lifted-SAE model demonstrate at least a 15% improvement in recovery of smooth concepts compared to traditional SAEs in benchmark tests?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.7,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2509.03738v2",
    "paper_title": "Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces",
    "pdf_url": "http://arxiv.org/pdf/2509.03738v2",
    "safety_reasoning": "This paper explores the mechanistic interpretability of neural operators, which is crucial for understanding AI decision-making processes. Improved interpretability can lead to safer AI systems by enabling better understanding and control over their behavior.",
    "price_reasoning": "The bid/ask prices reflect a moderate confidence in the lifted-SAE's performance based on the claims made in the paper. The spread indicates some uncertainty regarding the specific benchmark tests and conditions under which the improvement will be measured.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-01, will MechLight be demonstrated to outperform at least two other HE methods in explaining context utilisation across three distinct datasets?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.75,
    "ask_price": 0.82,
    "paper_url": "http://arxiv.org/abs/2510.02629v2",
    "paper_title": "Evaluation Framework for Highlight Explanations of Context Utilisation\n  in Language Models",
    "pdf_url": "http://arxiv.org/pdf/2510.02629v2",
    "safety_reasoning": "This paper addresses the transparency of language models, which is crucial for understanding their decision-making processes. Improved context utilisation explanations can enhance user trust and mitigate risks associated with AI misinterpretations.",
    "price_reasoning": "The bid price reflects a strong confidence in MechLight's performance based on the paper's findings, while the ask price accounts for potential variability in future evaluations and the challenges of context length. The spread indicates moderate uncertainty about the consistency of results across different datasets.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-30, will MLLMs demonstrate a human-like pop-out effect in visual search tasks with a statistically significant p-value of less than 0.05?",
    "resolution_date": "2025-10-30",
    "bid_price": 0.75,
    "ask_price": 0.82,
    "paper_url": "http://arxiv.org/abs/2510.19678v1",
    "paper_title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
    "pdf_url": "http://arxiv.org/pdf/2510.19678v1",
    "safety_reasoning": "This paper explores the perceptual capabilities of MLLMs, which is crucial for understanding their decision-making processes and potential biases. Evaluating their visual processing can help ensure that these models operate safely and effectively in real-world applications.",
    "price_reasoning": "The bid and ask prices reflect a moderate level of confidence in the findings of the paper, given the experimental evidence provided. However, there remains some uncertainty about the reproducibility of these results across different MLLMs and visual contexts, justifying a wider spread.",
    "keywords": "interpretability"
  },
  {
    "market_title": "By 2025-10-01, will the interpretability techniques applied in TinySQL demonstrate a statistically significant improvement in SQL generation accuracy compared to baseline models?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.65,
    "ask_price": 0.7,
    "paper_url": "http://arxiv.org/abs/2503.12730v5",
    "paper_title": "TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic\n  Interpretability Research",
    "pdf_url": "http://arxiv.org/pdf/2503.12730v5",
    "safety_reasoning": "This paper addresses the interpretability of AI models, which is crucial for understanding their decision-making processes. Improved interpretability can lead to safer AI systems by enabling developers to identify and mitigate potential biases or errors in model behavior.",
    "price_reasoning": "The bid and ask prices reflect a moderate level of confidence in the outcome, as the techniques proposed are innovative but still require empirical validation. The spread indicates some uncertainty regarding the effectiveness of the methods in practical applications.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-01, will Stream demonstrate a reduction of at least 90% in token interactions while preserving critical retrieval paths in LLMs on the RULER benchmark?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.75,
    "ask_price": 0.85,
    "paper_url": "http://arxiv.org/abs/2510.19875v1",
    "paper_title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
    "pdf_url": "http://arxiv.org/pdf/2510.19875v1",
    "safety_reasoning": "This paper addresses the challenge of mechanistic interpretability in large language models, which is crucial for understanding AI decision-making processes. Improved interpretability can enhance transparency and trust in AI systems, thereby contributing to safer AI deployment.",
    "price_reasoning": "The bid price reflects a strong confidence in the method's effectiveness based on the results presented, while the ask price accounts for potential variability in outcomes when applied to different contexts or benchmarks. The spread indicates some uncertainty due to the complexity of LLMs and the evolving nature of interpretability techniques.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-01, will rule-based descriptions of attention features in transformers be implemented in a widely used NLP model?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.65,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2510.18148v1",
    "paper_title": "Extracting Rule-based Descriptions of Attention Features in Transformers",
    "pdf_url": "http://arxiv.org/pdf/2510.18148v1",
    "safety_reasoning": "This paper contributes to AI safety by enhancing the interpretability of transformer models, which are widely used in NLP applications. Improved interpretability can help identify and mitigate biases in AI systems, leading to safer deployments.",
    "price_reasoning": "The bid price reflects a moderate confidence that rule-based descriptions will gain traction in the NLP community, while the ask price accounts for the uncertainty regarding the adoption timeline and practical implementation challenges.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-01, will the role-play prompt formulation improve the relevance ranking accuracy of zero-shot LLMs by at least 15% as measured by standard evaluation metrics?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.65,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2510.17535v1",
    "paper_title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
    "pdf_url": "http://arxiv.org/pdf/2510.17535v1",
    "safety_reasoning": "Understanding how role-play influences LLM behavior is crucial for ensuring that these models provide reliable outputs. This research contributes to interpretability and robustness, which are key aspects of AI safety, as they help mitigate risks associated with unpredictable model behavior.",
    "price_reasoning": "The bid price reflects a moderate confidence in the findings of the paper, while the ask price indicates a belief in the potential for significant improvement in LLM ranking accuracy. The spread accounts for uncertainty in the practical implementation and evaluation of role-play prompts.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-31, will a significant increase in trust among healthcare professionals in large language model explanations be demonstrated through a peer-reviewed study?",
    "resolution_date": "2025-10-31",
    "bid_price": 0.65,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2510.17256v1",
    "paper_title": "Explainability of Large Language Models: Opportunities and Challenges\n  toward Generating Trustworthy Explanations",
    "pdf_url": "http://arxiv.org/pdf/2510.17256v1",
    "safety_reasoning": "This paper addresses the explainability of large language models, which is crucial for their safe deployment in sensitive areas like healthcare. Understanding how these models generate predictions can help mitigate risks associated with erroneous outputs, thereby enhancing trust and safety in AI applications.",
    "price_reasoning": "The bid price reflects a moderate confidence that the research will lead to demonstrable trust improvements in healthcare settings, while the ask price accounts for the uncertainty surrounding the timeline and the complexity of conducting peer-reviewed studies in this domain.",
    "keywords": "explainability, interpretability"
  },
  {
    "market_title": "By 2025-10-01, will the ablation of high-discriminating neurons in GPT-2 lead to a statistically significant improvement in literary style metrics as defined in this paper?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.75,
    "ask_price": 0.8,
    "paper_url": "http://arxiv.org/abs/2510.17909v1",
    "paper_title": "Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in\n  Neural Language Models",
    "pdf_url": "http://arxiv.org/pdf/2510.17909v1",
    "safety_reasoning": "This paper highlights the disconnect between neuron activation and output quality in neural networks, which is crucial for understanding AI behavior. Misinterpretation of neuron functions could lead to unsafe AI systems if not properly addressed.",
    "price_reasoning": "The bid price reflects a moderate confidence in the outcome based on the paper's findings, while the ask price indicates a slight uncertainty due to potential variations in experimental conditions and interpretations of 'literary style metrics'.",
    "keywords": "alignment, interpretability, mechanistic analysis, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-15, will state-of-the-art vision-language models achieve at least 75% accuracy on binary humor classification using the HumorDB dataset?",
    "resolution_date": "2025-10-15",
    "bid_price": 0.65,
    "ask_price": 0.72,
    "paper_url": "http://arxiv.org/abs/2406.13564v3",
    "paper_title": "HumorDB: Can AI understand graphical humor?",
    "pdf_url": "http://arxiv.org/pdf/2406.13564v3",
    "safety_reasoning": "Understanding humor is a complex cognitive task that involves interpreting context and subtle cues. If AI systems can accurately classify humor, it could lead to safer and more effective human-AI interactions, reducing the risk of misunderstandings in communication.",
    "price_reasoning": "The bid price reflects a moderate confidence that advancements in AI will lead to improved performance on this task, while the ask price indicates a belief that there are still significant challenges to overcome. The spread suggests a reasonable level of uncertainty regarding the timeline and capability of AI systems in this area.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-01, will Transformer networks using the proposed architectural mechanisms achieve at least 80% accuracy on the GSM8K-style modular arithmetic task in out-of-distribution settings?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.75,
    "ask_price": 0.8,
    "paper_url": "http://arxiv.org/abs/2510.14095v1",
    "paper_title": "Unlocking Out-of-Distribution Generalization in Transformers via\n  Recursive Latent Space Reasoning",
    "pdf_url": "http://arxiv.org/pdf/2510.14095v1",
    "safety_reasoning": "This paper addresses the challenge of out-of-distribution generalization, which is crucial for ensuring that AI systems can perform reliably in real-world scenarios that differ from their training environments. Enhancing the reasoning capabilities of language models can lead to safer AI applications by reducing the likelihood of unexpected behaviors.",
    "price_reasoning": "The bid price reflects a moderate confidence that the proposed mechanisms will yield significant improvements in OOD performance, while the ask price accounts for potential challenges in achieving the specified accuracy. The spread indicates some uncertainty regarding the practical implementation and effectiveness of the proposed methods.",
    "keywords": "interpretability"
  },
  {
    "market_title": "By 2025-10-01, will the constrained Bayesian belief updating mechanism in transformers be experimentally validated through independent research?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.7,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2502.01954v2",
    "paper_title": "Constrained belief updates explain geometric structures in transformer\n  representations",
    "pdf_url": "http://arxiv.org/pdf/2502.01954v2",
    "safety_reasoning": "This paper explores the underlying mechanisms of transformers, which are widely used in AI systems. Understanding these mechanisms is crucial for ensuring that AI behaves in predictable and safe ways, particularly in applications involving decision-making and predictions.",
    "price_reasoning": "The bid price reflects a moderate confidence that independent validation of the constrained Bayesian belief updating mechanism will occur within the specified timeframe. The ask price indicates a slightly higher uncertainty, as the complexity of the research and potential variations in interpretation could affect the outcome.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-30, will RADAR achieve at least 90% accuracy in detecting data contamination in LLMs across a diverse evaluation set?",
    "resolution_date": "2025-10-30",
    "bid_price": 0.85,
    "ask_price": 0.9,
    "paper_url": "http://arxiv.org/abs/2510.08931v1",
    "paper_title": "RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM\n  Evaluation",
    "pdf_url": "http://arxiv.org/pdf/2510.08931v1",
    "safety_reasoning": "This paper addresses the critical issue of data contamination in LLM evaluation, which can lead to misleading assessments of model capabilities. By improving the detection of such contamination, the RADAR framework contributes to more reliable AI systems, enhancing overall safety and trust in AI technologies.",
    "price_reasoning": "The bid price reflects a strong confidence in RADAR's performance based on its reported 93% accuracy, while the ask price accounts for potential challenges in varied real-world applications, hence a narrower spread indicating moderate uncertainty.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-15, will the empirical NTK successfully identify features in a new neural network model trained on a complex dataset?",
    "resolution_date": "2025-10-15",
    "bid_price": 0.75,
    "ask_price": 0.8,
    "paper_url": "http://arxiv.org/abs/2510.00468v2",
    "paper_title": "Feature Identification via the Empirical NTK",
    "pdf_url": "http://arxiv.org/pdf/2510.00468v2",
    "safety_reasoning": "This paper contributes to AI safety by enhancing our understanding of how neural networks identify and utilize features, which is crucial for interpretability. Improved interpretability can help mitigate risks associated with opaque AI decision-making processes.",
    "price_reasoning": "The bid price reflects a strong confidence in the empirical NTK's applicability to new models, based on the paper's findings. The ask price accounts for potential variability in results across different datasets and architectures, indicating some uncertainty in the generalization of the results.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2026-10-15, will the parallel ensemble method proposed in this paper achieve at least a 5% improvement over the best baseline in the BlackboxNLP 2025 MIB Shared Task?",
    "resolution_date": "2026-10-15",
    "bid_price": 0.7,
    "ask_price": 0.75,
    "paper_url": "http://arxiv.org/abs/2510.06811v1",
    "paper_title": "BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for\n  Circuit Localization Methods",
    "pdf_url": "http://arxiv.org/pdf/2510.06811v1",
    "safety_reasoning": "This paper addresses the mechanistic interpretability of large language models, which is crucial for understanding and mitigating risks associated with AI behavior. Improved circuit localization methods can enhance transparency and reliability in AI systems, contributing to safer AI deployment.",
    "price_reasoning": "The bid price reflects a moderate confidence in the effectiveness of the proposed ensemble methods based on the results observed in the study. The ask price indicates a slight increase in uncertainty due to potential variations in performance across different model-task combinations.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-15, will the implementation of Cliff-as-a-Judge reduce attack success rates in poorly-aligned reasoning models to below 10%?",
    "resolution_date": "2025-10-15",
    "bid_price": 0.75,
    "ask_price": 0.82,
    "paper_url": "http://arxiv.org/abs/2510.06036v1",
    "paper_title": "Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?",
    "pdf_url": "http://arxiv.org/pdf/2510.06036v1",
    "safety_reasoning": "This paper addresses critical safety vulnerabilities in large reasoning models, particularly how misalignment can lead to harmful outputs. Understanding and mitigating these risks is essential for ensuring the safe deployment of AI systems in real-world applications.",
    "price_reasoning": "The bid price reflects a strong confidence in the proposed method's effectiveness based on the findings of the paper, while the ask price accounts for potential challenges in implementation and variability in model behavior. The spread indicates moderate uncertainty about the practical application of the proposed solution.",
    "keywords": "alignment, interpretability, safety"
  },
  {
    "market_title": "By 2025-10-01, will the Logical Implication Model Steering (LIMS) method demonstrate a statistically significant improvement in the interpretability of transformer models compared to baseline models?",
    "resolution_date": "2025-10-01",
    "bid_price": 0.65,
    "ask_price": 0.72,
    "paper_url": "http://arxiv.org/abs/2502.03618v2",
    "paper_title": "The Logical Implication Steering Method for Conditional Interventions on\n  Transformer Generation",
    "pdf_url": "http://arxiv.org/pdf/2502.03618v2",
    "safety_reasoning": "This paper addresses the interpretability of AI models, which is crucial for understanding and mitigating risks associated with AI behavior. By enhancing the transparency of model decisions, it contributes to safer AI deployment in sensitive applications.",
    "price_reasoning": "The bid and ask prices reflect a moderate confidence in the method's potential impact, given the promising theoretical foundation. However, the uncertainty surrounding practical implementation and validation leads to a wider spread.",
    "keywords": "interpretability, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-15, will the findings of this paper demonstrate that in-context learning (ICL) in large-scale Transformer models significantly exceeds memorization by at least 20% on a standardized benchmark?",
    "resolution_date": "2025-10-15",
    "bid_price": 0.65,
    "ask_price": 0.72,
    "paper_url": "http://arxiv.org/abs/2505.11004v3",
    "paper_title": "Illusion or Algorithm? Investigating Memorization, Emergence, and\n  Symbolic Processing in In-Context Learning",
    "pdf_url": "http://arxiv.org/pdf/2505.11004v3",
    "safety_reasoning": "This paper investigates the mechanisms behind in-context learning in large language models, which is crucial for understanding their capabilities and limitations. Insights from this research can inform the development of safer AI systems by clarifying how these models process and generate information.",
    "price_reasoning": "The bid price reflects a moderate confidence that the paper's findings will be validated through standardized benchmarks, while the ask price accounts for the uncertainty surrounding the interpretation of ICL mechanisms. The spread indicates a reasonable level of uncertainty regarding the extent to which ICL can be quantitatively distinguished from memorization.",
    "keywords": "interpretability, mechanistic analysis, mechanistic interpretability"
  },
  {
    "market_title": "By 2025-10-15, will the categorical framework proposed in this paper successfully unify at least three existing causal abstraction methods as demonstrated in a peer-reviewed publication?",
    "resolution_date": "2025-10-15",
    "bid_price": 0.65,
    "ask_price": 0.7,
    "paper_url": "http://arxiv.org/abs/2510.05033v1",
    "paper_title": "Causal Abstractions, Categorically Unified",
    "pdf_url": "http://arxiv.org/pdf/2510.05033v1",
    "safety_reasoning": "This paper contributes to AI safety by providing a structured approach to understanding causal relationships at different levels of abstraction, which is crucial for interpreting AI decision-making processes. A better understanding of causal models can help prevent unintended consequences in AI systems.",
    "price_reasoning": "The bid price reflects a moderate confidence that the framework will be validated through peer-reviewed research, while the ask price accounts for the uncertainty in the adoption and acceptance of the proposed methods within the academic community. The spread indicates a reasonable level of uncertainty due to the complexity of the topic.",
    "keywords": "interpretability"
  }
]